{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/atlas2/u/haotiany/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from guided_diffusion.script_util import (\n",
    "    NUM_CLASSES,\n",
    "    model_and_diffusion_defaults,\n",
    "    classifier_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    create_classifier,\n",
    "    add_dict_to_argparser,\n",
    "    args_to_dict,\n",
    "    ResNet18_32x32,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_argparser():\n",
    "    defaults = dict(\n",
    "        clip_denoised=True,\n",
    "        num_samples=10000,\n",
    "        batch_size=16,\n",
    "        use_ddim=False,\n",
    "        use_ddjm=False,\n",
    "        model_path=\"\",\n",
    "        log_dir=\"tmp\",\n",
    "        classifier_path=\"\",\n",
    "        guide_mode=\"None\",\n",
    "        classifier_scale=0.0,\n",
    "        positive_label=\"None\",\n",
    "        progress=False,\n",
    "        eta=0.0,\n",
    "        ref_batch=None,\n",
    "        test_classifier_path=\"\",\n",
    "        model_id=None,\n",
    "        iteration=10,\n",
    "        shrink_cond_x0=True,    # whether to shrink the score of x0 by at\n",
    "        faceid_loss_type='cosine',\n",
    "        face_image1_id='00000',\n",
    "        face_image2_id='00000',\n",
    "        face_image3_id='00000',\n",
    "        plot_args=False,\n",
    "        score_norm=1e09,\n",
    "        plot_traj=False,\n",
    "    )\n",
    "    defaults.update(model_and_diffusion_defaults())\n",
    "    defaults.update(classifier_defaults())\n",
    "    \n",
    "    return defaults\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # add_dict_to_argparser(parser, defaults)\n",
    "    # return parser\n",
    "\n",
    "args = create_argparser()\n",
    "# change dict to args, write your own code\n",
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "args = Args(**args)\n",
    "args.image_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TModel(\n",
       "  (model): EncoderUNetModel(\n",
       "    (time_embed): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (input_blocks): ModuleList(\n",
       "      (0): TimestepEmbedSequential(\n",
       "        (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttentionLegacy()\n",
       "          (proj_out): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttentionLegacy()\n",
       "          (proj_out): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (3): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Downsample(\n",
       "            (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          )\n",
       "          (x_upd): Downsample(\n",
       "            (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttentionLegacy()\n",
       "          (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (5): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttentionLegacy()\n",
       "          (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (6): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Downsample(\n",
       "            (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          )\n",
       "          (x_upd): Downsample(\n",
       "            (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "      (7): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttentionLegacy()\n",
       "          (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (8): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttentionLegacy()\n",
       "          (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (9): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Downsample(\n",
       "            (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          )\n",
       "          (x_upd): Downsample(\n",
       "            (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "      (10): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "      (11): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (middle_block): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttentionLegacy()\n",
       "        (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (out): Sequential(\n",
       "      (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "      (1): SiLU()\n",
       "      (2): AttentionPool2d(\n",
       "        (qkv_proj): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "        (c_proj): Conv1d(256, 1000, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npath = './ckpts/cifar_classifier/not_best_epoch96_acc0.9470.ckpt'\n",
    "nmodel = ResNet18_32x32()\n",
    "# load state dict\n",
    "nmodel.load_state_dict(torch.load(npath))\n",
    "\n",
    "\n",
    "class TModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(TModel, self).__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x, torch.tensor([0] * x.shape[0], device=x.device))\n",
    "\n",
    "tpath = './ckpts/cifar_classifier/model099999.pt'\n",
    "tmodel = create_classifier(**args_to_dict(args, classifier_defaults().keys()))\n",
    "# load state dict\n",
    "tmodel.load_state_dict(torch.load(tpath,))\n",
    "tmodel = TModel(tmodel)\n",
    "\n",
    "nmodel.eval()\n",
    "tmodel.eval()\n",
    "# to cuda\n",
    "nmodel.cuda()\n",
    "tmodel.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "label = 8\n",
    "# data_path = f'./evaluations/ref/cifar_test_{label}.npz'\n",
    "data_path = \"./working/witht_classifier_cifar/steps=200+pipe=ddim+iter=1+mode=manifold+scale=20.0+shrink=True/samples_1024x32x32x3.npz\"\n",
    "try:\n",
    "    data = np.load(data_path)\n",
    "except:\n",
    "    data_path = data_path.replace(\"samples_1024\", \"samples_64\")\n",
    "    data = np.load(data_path)\n",
    "# data.shape\n",
    "data = data['arr_0']\n",
    "data = data.transpose(0, 3, 1, 2) / 127.5 - 1\n",
    "data.min(), data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 32, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(data, dtype=torch.float32)\n",
    "x = x.cuda()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0')\n",
      "tensor(0.5938, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "btz = 64\n",
    "print(\n",
    "    (tmodel(x[:btz]).argmax(1) == label).float().mean()\n",
    ")\n",
    "print(\n",
    "    (nmodel(x[:btz]).argmax(1) == label).float().mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_grad(model, x, label):\n",
    "    x_in = x.detach().requires_grad_(True)\n",
    "    logits = model(x_in)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    selected = log_probs[:, label]\n",
    "    return torch.autograd.grad(selected.sum(), x_in)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3, 32, 32]), torch.Size([64, 3, 32, 32]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrad = get_grad(nmodel, x[:btz], label)\n",
    "tgrad = get_grad(tmodel, x[:btz], label)\n",
    "\n",
    "ngrad.shape, tgrad.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity of two tensors, where the first dimension is the batch dimension\n",
    "def cosine_similarity(a, b):\n",
    "    a, b = a.reshape(a.shape[0], -1), b.reshape(b.shape[0], -1)\n",
    "\n",
    "    return (a * b).sum(dim=-1) / (a.norm(dim=-1) * b.norm(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0084, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(ngrad, tgrad).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.9445, device='cuda:0'), tensor(0.7266, device='cuda:0'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrad.reshape(ngrad.shape[0], -1).norm(dim=-1).mean(), tgrad.reshape(tgrad.shape[0], -1).norm(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
