{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from guided_diffusion.script_util import (\n",
    "    NUM_CLASSES,\n",
    "    model_and_diffusion_defaults,\n",
    "    classifier_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    create_classifier,\n",
    "    add_dict_to_argparser,\n",
    "    args_to_dict,\n",
    "    ResNet18_32x32,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_argparser():\n",
    "    defaults = dict(\n",
    "        clip_denoised=True,\n",
    "        num_samples=10000,\n",
    "        batch_size=16,\n",
    "        use_ddim=False,\n",
    "        use_ddjm=False,\n",
    "        model_path=\"\",\n",
    "        log_dir=\"tmp\",\n",
    "        classifier_path=\"\",\n",
    "        guide_mode=\"None\",\n",
    "        classifier_scale=0.0,\n",
    "        positive_label=\"None\",\n",
    "        progress=False,\n",
    "        eta=0.0,\n",
    "        ref_batch=None,\n",
    "        test_classifier_path=\"\",\n",
    "        model_id=None,\n",
    "        iteration=10,\n",
    "        shrink_cond_x0=True,    # whether to shrink the score of x0 by at\n",
    "        faceid_loss_type='cosine',\n",
    "        face_image1_id='00000',\n",
    "        face_image2_id='00000',\n",
    "        face_image3_id='00000',\n",
    "        plot_args=False,\n",
    "        score_norm=1e09,\n",
    "        plot_traj=False,\n",
    "    )\n",
    "    defaults.update(model_and_diffusion_defaults())\n",
    "    defaults.update(classifier_defaults())\n",
    "    \n",
    "    return defaults\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # add_dict_to_argparser(parser, defaults)\n",
    "    # return parser\n",
    "\n",
    "args = create_argparser()\n",
    "# change dict to args, write your own code\n",
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "args = Args(**args)\n",
    "args.image_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TModel(\n",
       "  (model): EncoderUNetModel(\n",
       "    (time_embed): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (input_blocks): ModuleList(\n",
       "      (0): TimestepEmbedSequential(\n",
       "        (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttentionLegacy()\n",
       "          (proj_out): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttentionLegacy()\n",
       "          (proj_out): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (3): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Downsample(\n",
       "            (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          )\n",
       "          (x_upd): Downsample(\n",
       "            (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttentionLegacy()\n",
       "          (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (5): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttentionLegacy()\n",
       "          (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (6): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Downsample(\n",
       "            (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          )\n",
       "          (x_upd): Downsample(\n",
       "            (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "      (7): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttentionLegacy()\n",
       "          (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (8): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttentionLegacy()\n",
       "          (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (9): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Downsample(\n",
       "            (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          )\n",
       "          (x_upd): Downsample(\n",
       "            (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          )\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "      (10): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "      (11): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (middle_block): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttentionLegacy()\n",
       "        (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (out): Sequential(\n",
       "      (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "      (1): SiLU()\n",
       "      (2): AttentionPool2d(\n",
       "        (qkv_proj): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "        (c_proj): Conv1d(256, 1000, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npath = './ckpts/cifar_classifier/not_best_epoch96_acc0.9470.ckpt'\n",
    "nmodel = ResNet18_32x32()\n",
    "# load state dict\n",
    "nmodel.load_state_dict(torch.load(npath))\n",
    "\n",
    "\n",
    "class TModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(TModel, self).__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x, torch.tensor([0] * x.shape[0], device=x.device))\n",
    "\n",
    "tpath = './ckpts/cifar_classifier/model099999.pt'\n",
    "tmodel = create_classifier(**args_to_dict(args, classifier_defaults().keys()))\n",
    "# load state dict\n",
    "tmodel.load_state_dict(torch.load(tpath,))\n",
    "tmodel = TModel(tmodel)\n",
    "\n",
    "nmodel.eval()\n",
    "tmodel.eval()\n",
    "# to cuda\n",
    "nmodel.cuda()\n",
    "tmodel.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "label = 8\n",
    "data_path = f'./evaluations/ref/cifar_test_{label}.npz'\n",
    "data = np.load(data_path)\n",
    "# data.shape\n",
    "data = data['arr_0']\n",
    "data = data.transpose(0, 3, 1, 2) / 127.5 - 1\n",
    "data.min(), data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 3, 32, 32])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(data, dtype=torch.float32)\n",
    "x = x.cuda()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 8, 8, 8, 8, 7, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 7, 1, 8, 7, 8, 8, 8, 8, 8, 7, 8, 8, 8, 8, 8, 7, 7, 1,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0')\n",
      "tensor([8, 8, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 5, 8, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "btz = 64\n",
    "print(\n",
    "    tmodel(x[:btz]).argmax(1)\n",
    ")\n",
    "print(\n",
    "    nmodel(x[:btz]).argmax(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_grad(model, x, label):\n",
    "    x_in = x.detach().requires_grad_(True)\n",
    "    logits = model(x_in)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    selected = log_probs[:, label]\n",
    "    return torch.autograd.grad(selected.sum(), x_in)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3, 32, 32]), torch.Size([64, 3, 32, 32]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrad = get_grad(nmodel, x[:btz], label)\n",
    "tgrad = get_grad(tmodel, x[:btz], label)\n",
    "\n",
    "ngrad.shape, tgrad.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity of two tensors, where the first dimension is the batch dimension\n",
    "def cosine_similarity(a, b):\n",
    "    a, b = a.reshape(a.shape[0], -1), b.reshape(b.shape[0], -1)\n",
    "\n",
    "    return (a * b).sum(dim=-1) / (a.norm(dim=-1) * b.norm(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.2030e-02,  1.1297e-02,  1.6569e-02,  3.4075e-02,  4.2411e-02,\n",
       "        -1.2643e-02, -1.9589e-02,  1.7716e-02,  1.0036e-02, -4.3804e-02,\n",
       "         4.8255e-03, -7.6503e-03, -1.3355e-02,  1.8127e-02,  7.4011e-03,\n",
       "         1.8541e-02,  1.2955e-02, -3.2894e-02,  5.0961e-03,  1.7710e-02,\n",
       "         1.1894e-02, -5.5401e-03,  1.9137e-02,  4.0151e-05,  3.4603e-02,\n",
       "         2.6368e-02,  1.4285e-02,  1.7991e-02, -1.2071e-03,  1.6739e-02,\n",
       "         2.4171e-02,  7.2271e-03,  9.9153e-05,  2.2805e-02,  1.9722e-02,\n",
       "         3.6122e-03, -2.1280e-02,  4.5282e-03,  2.8151e-02, -1.7380e-02,\n",
       "         3.2525e-02, -1.2402e-03,  1.6555e-02,  3.6585e-03,  2.1830e-04,\n",
       "         2.3566e-02,  9.9438e-03,  1.3988e-02,  1.5145e-02, -6.2016e-03,\n",
       "        -1.7851e-02,  2.5497e-02, -3.3604e-02,  1.1986e-02,  9.5256e-03,\n",
       "        -6.9845e-03, -4.5876e-03,  8.0631e-03, -1.0670e-02, -4.4865e-03,\n",
       "         4.2157e-03,  1.7546e-04, -5.1217e-03, -8.2665e-04], device='cuda:0')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(ngrad, tgrad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
